{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOtYA9iRY1WcHcoU7/VE0K1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdiA32/ImageAesthetic/blob/main/Aesthetic_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "9fmWKV2aKLqF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dan5tz-mHPLZ",
        "outputId": "6b637abc-0b02-4325-9f76-35d958b83910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-vision in /usr/local/lib/python3.10/dist-packages (3.4.5)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision) (2.11.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision) (1.22.3)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision) (3.20.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (1.61.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (2.17.3)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (2.31.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (1.59.2)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (4.9)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (2023.7.22)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (0.5.0)\n",
            "Requirement already satisfied: mysql-connector-python-rf in /usr/local/lib/python3.10/dist-packages (2.2.2)\n"
          ]
        }
      ],
      "source": [
        "# Installs\n",
        "\n",
        "!pip install --upgrade google-cloud-vision\n",
        "!pip install mysql-connector-python-rf\n",
        "!pip install -qU transformers torch datasets gdcm pydicom pinecone-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Imports\n",
        "\n",
        "import os\n",
        "from google.cloud import vision\n",
        "import mysql.connector\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine, text\n",
        "\n",
        "# Embedding Imports\n",
        "import torch\n",
        "import skimage\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import pinecone\n",
        "import IPython.display as display\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "from collections import OrderedDict\n",
        "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import json\n",
        "import os\n",
        "import pinecone\n",
        "import openai\n",
        "import requests\n",
        "import numpy as np\n",
        "import itertools\n",
        "import torch\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from dotenv import load_dotenv\n",
        "from bs4 import BeautifulSoup\n",
        "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
        "from termcolor import colored\n",
        "from urllib.parse import urlparse\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from openai import OpenAI\n",
        "from PIL import Image\n",
        "from imageio import imread\n",
        "from io import BytesIO\n",
        "from google.colab import userdata\n"
      ],
      "metadata": {
        "id": "7HHwJJRlHhix",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "6528a992-0010-441f-9753-446220feea87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-0342805356b7>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCLIPProcessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIPModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIPTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Ensure that torch.ops.torchvision is visible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/extension.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0m_check_cuda_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/extension.py\u001b[0m in \u001b[0;36m_check_cuda_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mt_minor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_version\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt_major\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtv_major\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m     79\u001b[0m                 \u001b[0;34m\"Detected that PyTorch and torchvision were compiled with different CUDA major versions. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;34mf\"PyTorch has CUDA Version={t_major}.{t_minor} and torchvision has \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Detected that PyTorch and torchvision were compiled with different CUDA major versions. PyTorch has CUDA Version=12.1 and torchvision has CUDA Version=11.8. Please reinstall the torchvision that matches your PyTorch install."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Pinecone\n",
        "pinecone.init(api_key=userdata.get('PINECONE_API_KEY'), environment=\"gcp-starter\")\n",
        "\n",
        "# Create a Pinecone index\n",
        "index_name = 'aesthetic'\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    pinecone.create_index(index_name, dimension=512)\n",
        "\n",
        "# Connect to your index\n",
        "index = pinecone.Index(index_name)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Need openaikey\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "client = OpenAI()\n",
        "model_id = \"openai/clip-vit-base-patch32\"\n",
        "\n",
        "processor = CLIPProcessor.from_pretrained(model_id)\n",
        "model = CLIPModel.from_pretrained(model_id)\n",
        "\n",
        "# move model to device if possible\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "out = model.to(device)"
      ],
      "metadata": {
        "id": "rhRFOoV3LYVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a Dataset of 10 Aesthetics\n",
        "- Select 10 different fashion aesthetics (e.g., streetwear, vintage, boho, etc.).\n",
        "- For each aesthetic, gather a minimum of 10 images (5 representing male fashion and 5 representing female fashion).\n",
        "- Ensure these images are diverse and accurately represent the chosen aesthetic. singles?\n"
      ],
      "metadata": {
        "id": "wubMLsXyJ7aK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "styles = [\n",
        "    \"Classic Businesswear\",\n",
        "    \"Streetwear\",\n",
        "    \"Preppy\",\n",
        "    \"Minimalist\",\n",
        "    \"Skater\",\n",
        "    \"Soft\",\n",
        "    \"Athleisure\",\n",
        "    \"Boho (Bohemian)\",\n",
        "    \"Vintage\",\n",
        "    \"Athletic\"\n",
        "]\n",
        "\n",
        "#added gender so i can build better database\n",
        "def get_styles(query, gender, num_results=5):\n",
        "    url = \"https://www.googleapis.com/customsearch/v1\"\n",
        "\n",
        "    full_query = f\"{query} {gender} fashion\"  # Include gender in the query\n",
        "\n",
        "    params = {\n",
        "        \"q\": full_query,\n",
        "        \"cx\": userdata.get('GOOGLE_CSE_ID'),\n",
        "        \"key\": userdata.get('GOOGLE_API_KEY'),\n",
        "        \"searchType\": \"image\",\n",
        "        \"num\": num_results\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return [item[\"link\"] for item in response.json()[\"items\"]]\n",
        "    else:\n",
        "        print(\"Error: \", response.status_code)\n",
        "        return []\n",
        "\n",
        "#calling ^ and getting 5 women and 5 men items for each style\n",
        "def get_style_images(styles, num_results_per_gender=5):\n",
        "    images = []\n",
        "    for style in styles:\n",
        "        for gender in [\"men\", \"women\"]:\n",
        "            try:\n",
        "                temp = get_styles(style, gender, num_results_per_gender)\n",
        "            except Exception as e:\n",
        "                print(f\"Error occurred for style: {style}, gender: {gender}, error: {e}\")\n",
        "                temp = []\n",
        "            image = {\n",
        "                \"style\": style,\n",
        "                \"gender\": gender,\n",
        "                \"images\": temp\n",
        "            }\n",
        "            images.append(image)\n",
        "    return images\n",
        "\n",
        "\n",
        "def download_images(image, base_folder, verbose=False):\n",
        "    style = image['style']\n",
        "    images = image['images']\n",
        "    style_folder = Path(base_folder) / style\n",
        "\n",
        "    # Create the folder for the style if it doesn't exist\n",
        "    style_folder.mkdir(parents=True, exist_ok=True)\n",
        "    output = []\n",
        "\n",
        "    for url in images:\n",
        "        try:\n",
        "            # Parse the URL to get the image name\n",
        "            image_name = Path(urlparse(url).path).name\n",
        "            image_path = style_folder / image_name\n",
        "\n",
        "            # Download the image\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:\n",
        "                with open(image_path, 'wb') as file:\n",
        "                    file.write(response.content)\n",
        "                    output.append({\"style\": style, \"image_name\": image_name, \"image_path\": image_path, \"url\": url})\n",
        "                if verbose:\n",
        "                    print(f\"Image saved at {image_path}\")\n",
        "            else:\n",
        "                print(f\"Failed to download {url}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error occurred: {e}\")\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "BNgpC4rIAYd8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = get_style_images(styles)"
      ],
      "metadata": {
        "id": "hPqMrrOmGvKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for image in images:\n",
        "    print(image[\"style\"])\n",
        "    for img in image[\"images\"]:\n",
        "        print(img)\n",
        "    print()"
      ],
      "metadata": {
        "id": "QGhNwfVhAZ4T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "738351a5-cd8a-40ba-dd22-09fea223f39f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classic Businesswear\n",
            "https://m.media-amazon.com/images/I/61aT+97zQWL._AC_UY1000_.jpg\n",
            "https://i.pinimg.com/564x/71/21/4a/71214a29ea3510acd739cd603f3c6f1a.jpg\n",
            "https://m.media-amazon.com/images/I/81MWYrJ8BrL._AC_UY1000_.jpg\n",
            "https://i.pinimg.com/736x/72/1b/4d/721b4d958ac1e7eaf304789f1577f16e.jpg\n",
            "https://m.media-amazon.com/images/I/61i3bzLKIfL._AC_UF894,1000_QL80_.jpg\n",
            "\n",
            "Classic Businesswear\n",
            "https://d2w9m16hs9jc37.cloudfront.net/dimg/blog/2021/06/278a0084.jpg\n",
            "https://i.pinimg.com/736x/cd/68/ea/cd68eab11e7b62bfcb67f57e2db912bf.jpg\n",
            "https://i5.walmartimages.com/seo/YOTAMI-Dress-for-Women-2022-Autumn-and-Winter-Long-Sleeve-Pullver-Classic-Loose-Fashion-Casual-Deals-under-15-Blue-S-6XL_dbf1a423-6b0d-4ca0-b554-badd51dfd19c.a705425ca0bf2bbd9cdfe770811b21d0.jpeg?odnHeight=768&odnWidth=768&odnBg=FFFFFF\n",
            "https://i.pinimg.com/736x/7a/f8/21/7af82197ab34029376a0ce88a68eac0e.jpg\n",
            "https://i5.walmartimages.com/seo/YOTAMI-Dress-for-Women-2022-Autumn-and-Winter-Long-Sleeve-Pullver-Classic-Loose-Fashion-Casual-Deals-under-15-Black-S-6XL_473edb8d-358c-46a1-b6eb-c5833dcb67ee.d045c108eefc3c0773546fe3dad63deb.jpeg?odnHeight=768&odnWidth=768&odnBg=FFFFFF\n",
            "\n",
            "Streetwear\n",
            "https://i.pinimg.com/550x/49/ed/99/49ed99d0440c65c57a8b33d3a17551d6.jpg\n",
            "https://mensflair.com/wp-content/uploads/2023/03/mens-streetwear-outfit-9-900x1350.jpg\n",
            "https://i.pinimg.com/474x/e8/6a/a3/e86aa3e3eb53133a7403d3c7fee0cb4c.jpg\n",
            "https://assets.vogue.com/photos/60d88c184b12a701e3d2f114/4:3/w_2664,h_1998,c_limit/Paris%20Mens%20SS22%20day%205%20by%20STYLEDUMONDE%20Street%20Style%20Fashion%20Photography_95A0608FullRes.jpg\n",
            "https://i.pinimg.com/736x/63/f1/cb/63f1cbf893332b75dc2757c9af81c074.jpg\n",
            "\n",
            "Streetwear\n",
            "https://i.pinimg.com/736x/0d/a3/61/0da36176d27bf360e3b730ccc60e7166.jpg\n",
            "https://www.yourcoffeebreak.co.uk/wp-content/uploads/d9a65be8051b1c25035c63728aea888f.jpg\n",
            "https://i.pinimg.com/736x/33/dd/60/33dd602681714b7edd402424317f9a99.jpg\n",
            "https://static.fibre2fashion.com//articleresources/images/89/8871/sm_Small.jpg\n",
            "https://i.pinimg.com/564x/a6/83/9d/a6839d9a476bab8c38c0154dfd343ac8.jpg\n",
            "\n",
            "Preppy\n",
            "https://www.stitchfix.com/men/blog/wp-content/uploads/2022/03/21-12-15_M_OF_V01_0689_2x3-scaled.jpeg\n",
            "https://i.pinimg.com/564x/8d/71/cd/8d71cda6815039619c871622a4f060bd.jpg\n",
            "https://media.gq-magazine.co.uk/photos/5e413a9cbe85dd00081a9d46/master/pass/TRYP.jpg\n",
            "https://i.pinimg.com/originals/31/56/55/315655d27c2ec3a04b18db4e34013658.png\n",
            "https://www.ties.com/blog/wp-content/uploads/2018/10/J-Crew-Sweater-Vest.jpg\n",
            "\n",
            "Preppy\n",
            "https://i.pinimg.com/736x/87/be/bb/87bebb8a9bd09354d758de4fae8b5df0.jpg\n",
            "https://media.glamour.com/photos/594a88e1c750c9223d307282/master/w_2560%2Cc_limit/Lede_streetstyle_preppy.jpg\n",
            "https://i.pinimg.com/originals/87/be/bb/87bebb8a9bd09354d758de4fae8b5df0.jpg\n",
            "https://40plusstyle.com/wp-content/uploads/2019/09/prepstylecelebrities.jpg\n",
            "https://i.pinimg.com/564x/91/df/a1/91dfa16495598ff26d2de415e7a78f8f.jpg\n",
            "\n",
            "Minimalist\n",
            "https://i.ytimg.com/vi/SVZ_vKVieK8/maxresdefault.jpg\n",
            "https://www.thegoodtrade.com/wp-content/uploads/2023/01/minimalist-men-fashion-instagrammers.jpg\n",
            "https://i.ytimg.com/vi/liZUbYjWEBw/maxresdefault.jpg\n",
            "https://www.primermagazine.com/wp-content/uploads/2018/08/minimalist-fashion-tall-1.jpg\n",
            "https://i.pinimg.com/originals/b6/eb/31/b6eb315e17ed030700a69b66b45f4a71.jpg\n",
            "\n",
            "Minimalist\n",
            "https://4.bp.blogspot.com/--H646dtWJi0/WgxKvsICR_I/AAAAAAAAXzk/EmFwzcDSe9obYV82t_GWMu6dgkPEXrwCACLcBGAs/s1600/9%2BMinimalist%2BStyle%2BFashion%2BBloggers%2BYou%2BShould%2BKnow%2B001.jpg\n",
            "https://hips.hearstapps.com/hmg-prod/images/index-2-1608755610.jpg?crop=0.493xw:0.990xh;0.252xw,0&resize=640:*\n",
            "https://assets.vogue.com/photos/5e2b0afa540d7f000847c4a0/master/w_2560%2Cc_limit/VO0220_Minimalism_12.jpg\n",
            "https://stylecaster.com/wp-content/uploads/2021/08/Paris-HC-str-S22-0549.jpg?w=447\n",
            "https://media1.popsugar-assets.com/files/thumbor/BxIwCt28RxVPOOICKtrt7INTY8I=/14x174:640x800/fit-in/640x800/filters:format_auto():upscale()/2020/01/17/702/n/1922564/dd97c95c5e21d831bd4992.65634751_.jpg\n",
            "\n",
            "Skater\n",
            "https://i.pinimg.com/564x/26/ed/18/26ed18dcc8e37e08d4cb588dd784a031.jpg\n",
            "https://cdn.shopify.com/s/files/1/0305/7100/4043/files/Skater_style_casual_look1_1024x1024.jpg?v=1677077144\n",
            "https://i.pinimg.com/originals/19/6c/d0/196cd0fd944cc48327a002e6807398b2.jpg\n",
            "https://s.wsj.net/public/resources/images/OD-BH044A_SKATE_8V_20150724144503.jpg\n",
            "https://i.pinimg.com/736x/64/a2/74/64a27473313a9df81d7af65ae23ef672.jpg\n",
            "\n",
            "Skater\n",
            "https://i.pinimg.com/564x/c1/fa/00/c1fa00727ba410034bdd0263f0f305f7.jpg\n",
            "https://assets.vogue.com/photos/5fbd418f7566c0912a137052/1:1/w_354%2Cc_limit/00-social-crop.jpg\n",
            "https://i.pinimg.com/736x/39/ed/ee/39edeeb69c4d9ca4492d7c3d6dedf2ba.jpg\n",
            "https://image-cdn.hypb.st/https%3A%2F%2Fbae.hypebeast.com%2Ffiles%2F2018%2F05%2Ftop-skate-clothing-brands-women-stussy-vans-dickies-carhartt-35.jpg?w=1260&format=jpeg&cbr=1&q=90&fit=max\n",
            "https://i.pinimg.com/736x/70/10/92/701092dc5cfaeff1122ecc8930559161.jpg\n",
            "\n",
            "Soft\n",
            "https://i.pinimg.com/564x/b2/e1/1d/b2e11da3b4f817697b29555206890b8b.jpg\n",
            "https://sourcingjournal.com/wp-content/uploads/2020/01/shutterstock_editorial_10524573fk.jpg?w=492&h=656&crop=1\n",
            "https://i.pinimg.com/564x/8d/fd/e3/8dfde348364e6b1192d9055b429b662c.jpg\n",
            "http://oystercoloredvelvet.com/wp-content/uploads/2015/06/IMG_4246.jpg\n",
            "https://i.pinimg.com/564x/ed/28/0e/ed280ec919475b99f688e44aebfbabb2.jpg\n",
            "\n",
            "Soft\n",
            "https://m.media-amazon.com/images/I/610a2CglQkL._AC_UF1000,1000_QL80_.jpg\n",
            "https://static01.nyt.com/images/2021/10/14/fashion/11gender-agnostic1/11gender-agnostic1-articleLarge-v2.jpg?quality=75&auto=webp&disable=upscale\n",
            "https://m.media-amazon.com/images/I/81EY5MQOnFL._AC_UY1000_.jpg\n",
            "https://i.pinimg.com/736x/f3/3c/49/f33c491b776c547418aaad2b05a90bf1.jpg\n",
            "https://m.media-amazon.com/images/I/61lYZiLvsaL._AC_UF1000,1000_QL80_.jpg\n",
            "\n",
            "Athleisure\n",
            "https://i.pinimg.com/736x/de/10/67/de1067e8a060d8cf7da3823fdeeae34f.jpg\n",
            "https://miro.medium.com/v2/resize:fit:736/1*Q26PfXpKXZj3hb1y0c0bnA.jpeg\n",
            "https://images.squarespace-cdn.com/content/v1/54661df4e4b0c1af99306b69/1496053272476-QQYNX2BCBGSP1U87CT71/165A5817.jpg\n",
            "https://i.pinimg.com/1200x/f1/a6/90/f1a690a88082a73658cce95694a4318a.jpg\n",
            "https://i1.wp.com/stylegirlfriend.com/wp-content/uploads/2020/12/gabe-collins-2.png?fit=1024%2C1024&ssl=1\n",
            "\n",
            "Athleisure\n",
            "https://static01.nyt.com/images/2015/11/05/fashion/05UNBUTTONED2/05UNBUTTONED2-articleLarge.jpg?quality=75&auto=webp&disable=upscale\n",
            "https://i2.wp.com/thefashionhousemom.com/wp-content/uploads/2020/04/8C4F42BD-889F-4EE0-95D0-7499A9CD3E86.jpg?resize=1140%2C1713&ssl=1\n",
            "https://i.pinimg.com/736x/05/2c/77/052c772d36bb297f9eb2a54ce94e2c7e.jpg\n",
            "https://assets.vogue.com/photos/63c1bc1598262bb815e76603/master/w_2560%2Cc_limit/LOEWE_SS23_PRECO_LOOKBOOK_LOOK_29_RGB_CROPPED_4X5_29.jpg\n",
            "https://i.pinimg.com/originals/58/e6/ff/58e6ff800f837cfa907b3e326e07fcba.jpg\n",
            "\n",
            "Boho (Bohemian)\n",
            "https://i.pinimg.com/736x/d6/f7/19/d6f71935fc2876b418f91d616bc868c1.jpg\n",
            "https://i.ytimg.com/vi/WB6z3ZQ_goQ/maxresdefault.jpg\n",
            "https://i.pinimg.com/564x/05/9f/3c/059f3c436a61bab3f0b3f8ef841e9b0e.jpg\n",
            "https://onpointfresh.com/wp-content/uploads/2021/12/%CE%A3%CF%87%CE%AD%CE%B4%CE%B9%CE%BF-%CF%87%CF%89%CF%81%CE%AF%CF%82-%CF%84%CE%AF%CF%84%CE%BB%CE%BF-2-4.png\n",
            "https://i.pinimg.com/564x/6c/b9/80/6cb980fa8d56be106cbb269d68c472c9--fashion-style-guide-mens-fashion-blog.jpg\n",
            "\n",
            "Boho (Bohemian)\n",
            "https://i.pinimg.com/736x/10/c1/b3/10c1b30c7e5d017acc8fc50ba9b29029.jpg\n",
            "https://www.ibizabohogirl.com/wp-content/uploads/2021/02/saltycrush888-min-600x899.jpg\n",
            "https://i.pinimg.com/564x/e2/6e/4d/e26e4d0f3621d04785bb32cff70f2c1e.jpg\n",
            "https://assets.vogue.com/photos/64947731956dd488a4432b65/master/w_1600%2Cc_limit/boho_dresses_holding.png\n",
            "https://i.pinimg.com/736x/82/e3/74/82e374fb0a5ae15a44cce9a4cf96e431.jpg\n",
            "\n",
            "Vintage\n",
            "https://i.pinimg.com/474x/84/2a/0d/842a0da9095ecbe9f56fca833409db96.jpg\n",
            "https://hips.hearstapps.com/hmg-prod/images/lead-1536274181.jpg?crop=0.732xw:0.983xh;0.00321xw,0&resize=768:*\n",
            "https://i.pinimg.com/474x/de/0b/85/de0b856ad6c15a2af8d675e81ede6539.jpg\n",
            "https://homobilia.com/cdn/shop/products/fashion-12_1024x1024.jpg?v=1645645532\n",
            "https://i.pinimg.com/736x/9f/4e/8e/9f4e8e6447d72678fa1f35600758d470.jpg\n",
            "\n",
            "Vintage\n",
            "https://i.pinimg.com/474x/14/f6/59/14f6591ad25f1bbb877167b8a5fe67d1.jpg\n",
            "https://dapperday.com/cdn/shop/products/71BB5024-6758-4C18-9C0A-86F51847ABB7-1200_530x@2x.jpg?v=1606431604\n",
            "https://i.pinimg.com/564x/15/7b/c1/157bc162d2b7f6eb8ae4d936bcabc833.jpg\n",
            "https://i.etsystatic.com/9537835/r/il/6e18e2/4218810794/il_fullxfull.4218810794_szv6.jpg\n",
            "https://mymodernmet.com/wp/wp-content/uploads/2016/12/modern-women-vintage-world-0.jpg\n",
            "\n",
            "Athletic\n",
            "https://i.pinimg.com/236x/f9/66/65/f966655227a70a316ce0d11eaf647556.jpg\n",
            "https://onpointfresh.com/wp-content/uploads/2017/04/26c545ff925ca969008400f7059f2135.jpg\n",
            "https://i.pinimg.com/originals/1c/1b/ce/1c1bce1c953618d012ca4995822450f6.jpg\n",
            "https://onpointfresh.com/wp-content/uploads/2017/04/26c545ff925ca969008400f7059f2135-e1493428976769.jpg\n",
            "https://i.pinimg.com/564x/1c/1b/ce/1c1bce1c953618d012ca4995822450f6.jpg\n",
            "\n",
            "Athletic\n",
            "https://m.media-amazon.com/images/I/61uTpoyOTZL._AC_UY1000_.jpg\n",
            "https://i.pinimg.com/736x/86/5a/dc/865adcfb82c126bd2da4dd12eecf0e94.jpg\n",
            "https://m.media-amazon.com/images/I/91f+GGpt1YL._AC_UY1000_.jpg\n",
            "https://i5.walmartimages.com/seo/Athletic-Works-Long-Sleeve-Active-Fit-Stripes-Fashion-Jacket-Women-s-1-Pack_d1540def-c137-452e-8f45-bad515cd9543_1.38e6cdf0b2a3a520bd5785ba0c0d0f32.jpeg?odnHeight=768&odnWidth=768&odnBg=FFFFFF\n",
            "https://img.joomcdn.net/0bb9df571aa3d99569ca832c5743ad821435e0cd_original.jpeg\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Embeddings Using OpenAI GPT-4V (Turbo Preview Model) and Upsert to Pinecone\n",
        "- Use the OpenAI GPT-4V (Turbo Preview model) to generate embeddings for each image. This model can create rich, descriptive embeddings that capture the essence of the images.\n",
        "- Once the embeddings are generated, upsert (update or insert) them into your Pinecone index along with relevant metadata (like aesthetic type, gender representation, etc.).\n"
      ],
      "metadata": {
        "id": "rcmmcRgtJ2D5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "Embeddings, in the context of machine learning and particularly with your project involving OpenAI's GPT-4V and Pinecone, are a way of representing complex data, like images, in a form that a computer can understand and process efficiently. Here’s a more detailed explanation:\n",
        "\n",
        "### What Are Embeddings?\n",
        "\n",
        "1. **Numerical Representations**: Embeddings are numerical representations of data in a high-dimensional space. For images, this means converting the visual content into a list (or vector) of numbers.\n",
        "\n",
        "2. **Capturing Essence**: These numerical vectors aim to capture the essence or key features of the data. For an image, this might include color schemes, shapes, textures, or even more abstract concepts like the style or mood of the image.\n",
        "\n",
        "3. **Dimensionality Reduction**: Embeddings often involve reducing the dimensionality of the original data. An image, which is essentially a large array of pixel values, is transformed into a much smaller vector that still retains the important information.\n",
        "\n",
        "### How Do They Work in Your Project?\n",
        "\n",
        "In your project, you use OpenAI's GPT-4V to generate embeddings for images. Here's how that fits in:\n",
        "\n",
        "1. **Image Processing**: When you pass an image to OpenAI's GPT-4V model, the model processes the image and extracts its key features.\n",
        "\n",
        "2. **Vector Creation**: The model then represents these features as a vector of numbers. This vector is the \"embedding\" of the image.\n",
        "\n",
        "3. **Uses of Embeddings**: These embeddings can be used for various purposes, such as:\n",
        "   - **Similarity Comparison**: Comparing vectors to find similar images (images with similar vectors are likely to be visually or stylistically similar).\n",
        "   - **Indexing and Retrieval**: In Pinecone, you store these embeddings so that you can later search through them efficiently. When you query the Pinecone index with an embedding, it retrieves items with similar embeddings.\n",
        "\n",
        "### Why Use Embeddings?\n",
        "\n",
        "1. **Efficiency**: Dealing with raw images directly is often computationally expensive and inefficient. Embeddings reduce the complexity of the data while retaining the essential information.\n",
        "\n",
        "2. **Machine Learning Readiness**: Embeddings are a form of feature extraction, making the data ready for machine learning models, which typically work with numbers.\n",
        "\n",
        "3. **Versatility**: The same embedding technique can be used for different types of data (like text, images, and audio), allowing for consistent processing and analysis methods.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "In summary, in your project, embeddings serve as a bridge between the rich, complex world of visual data and the numerical realm where machine learning algorithms operate. They allow you to efficiently store, search, and analyze images based on their content."
      ],
      "metadata": {
        "id": "MAgNt8OsL3Zl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retard terms:\n",
        "In simpler terms, embeddings are like a special code that represents something complex in a way that computers can understand and work with easily.\n",
        "\n",
        "Imagine you have a bunch of photos, and you want a computer to sort them based on their style. Photos are complicated — they have lots of colors, shapes, and details. A computer can't just look at a photo and understand it like we do. So, we turn each photo into an embedding, which is like a unique fingerprint for each photo.\n",
        "\n",
        "Here's how it works:\n",
        "\n",
        "1. **Turning Photos into Numbers**: A computer program looks at a photo and picks out its most important features, like colors, patterns, and shapes. Then, it turns these features into a list of numbers. This list of numbers is the embedding.\n",
        "\n",
        "2. **Comparing Photos**: Now, each photo has its own list of numbers. The computer can easily compare these lists to see which photos are similar. Photos with similar lists of numbers are likely to have similar styles.\n",
        "\n",
        "3. **Storing and Searching**: We can store these lists of numbers in a system. Later, if we want to find photos with a certain style, the computer can quickly search through these numbers and find what we're looking for.\n",
        "\n",
        "In your project, you're using a special tool from OpenAI to create these number lists for each photo. Then, you use another tool called Pinecone to store and search through these lists. This makes it easier to organize and find photos based on their style!"
      ],
      "metadata": {
        "id": "VrSOv9HWMS8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import base64\n",
        "\n",
        "# Your OpenAI API key\n",
        "openai.api_key = 'YOUR_OPENAI_API_KEY'\n",
        "\n",
        "def get_image_embedding(image_path):\n",
        "    \"\"\"\n",
        "    Generates an embedding for an image using OpenAI's API.\n",
        "\n",
        "    :param image_path: Local path to the image.\n",
        "    :return: Embedding of the image.\n",
        "    \"\"\"\n",
        "    with open(image_path, 'rb') as image_file:\n",
        "        image_base64 = base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "    response = openai.Image.create_embedding(\n",
        "        model=\"turbo-preview-model\",  # Replace with the appropriate model name\n",
        "        image_base64=image_base64\n",
        "    )\n",
        "\n",
        "    return response['data']\n"
      ],
      "metadata": {
        "id": "r4VU69i0JfYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uyjafPqmJf_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upsert_to_pinecone(id, embedding, metadata):\n",
        "    \"\"\"\n",
        "    Upserts data to a Pinecone index.\n",
        "\n",
        "    :param id: A unique identifier for the data.\n",
        "    :param embedding: The embedding vector.\n",
        "    :param metadata: Additional metadata to store with the embedding.\n",
        "    \"\"\"\n",
        "    index.upsert(vectors={id: (embedding, metadata)})\n"
      ],
      "metadata": {
        "id": "DCVQRWMFJgBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DocPRS0kJgEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3VlN4nnfJgJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pt48ZJfUJgLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a Query Function\n",
        "Develop a function that takes an image as input. This function should use Visual Question Answering (VQA) techniques to analyze the image.\n",
        "The function should classify the aesthetic of the outfit in the image based on its visual characteristics.\n",
        "Utilize machine learning models or AI services capable of understanding fashion-related queries for this purpose.\n"
      ],
      "metadata": {
        "id": "gee7sa-pKp4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "cool idea: show pic of outfit i am wearing and get aesthetic?"
      ],
      "metadata": {
        "id": "j4D4UlUtNFNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def classify_outfit_aesthetic(image_path, api_key):\n",
        "    \"\"\"\n",
        "    Classifies the aesthetic of an outfit in an image.\n",
        "\n",
        "    :param image_path: Path to the image file.\n",
        "    :param api_key: API key for the image analysis service.\n",
        "    :return: Predicted aesthetic of the outfit.\n",
        "    \"\"\"\n",
        "    # Load the image\n",
        "    with open(image_path, 'rb') as image_file:\n",
        "        image_data = image_file.read()\n",
        "\n",
        "    # Prepare the request for the API\n",
        "    # This part varies depending on the specific API you're using\n",
        "    headers = {'Authorization': f'Bearer {api_key}'}\n",
        "    payload = {'image': image_data}  # Adjust this according to the API's requirements\n",
        "\n",
        "    # Send the request to the API\n",
        "    response = requests.post('API_ENDPOINT', headers=headers, data=payload)\n",
        "\n",
        "    # Process the response\n",
        "    # The processing here will depend on the structure of the response from the API\n",
        "    if response.status_code == 200:\n",
        "        analysis_result = response.json()\n",
        "        # Extract the relevant information from the analysis_result to determine the aesthetic\n",
        "        # This might involve looking for specific keywords or features\n",
        "        aesthetic = extract_aesthetic_from_result(analysis_result)\n",
        "        return aesthetic\n",
        "    else:\n",
        "        print(f\"Error in API call: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "def extract_aesthetic_from_result(result):\n",
        "    \"\"\"\n",
        "    Extracts the aesthetic from the analysis result.\n",
        "\n",
        "    :param result: The result from the image analysis API.\n",
        "    :return: The determined aesthetic.\n",
        "    \"\"\"\n",
        "    # Implement logic to analyze the result and determine the aesthetic\n",
        "    # This could be based on identified objects, colors, styles, etc.\n",
        "    # For example:\n",
        "    if 'vintage' in result['description']:\n",
        "        return 'Vintage'\n",
        "    elif 'sporty' in result['description']:\n",
        "        return 'Athletic'\n",
        "    # Add more conditions as needed\n",
        "    else:\n",
        "        return 'Unknown'\n",
        "\n",
        "# Example usage\n",
        "api_key = 'YOUR_API_KEY'\n",
        "image_path = 'path/to/your/image.jpg'\n",
        "aesthetic = classify_outfit_aesthetic(image_path, api_key)\n",
        "print(f\"The outfit's aesthetic is: {aesthetic}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "-4WvBTyuJgN-",
        "outputId": "784d453e-d719-4942-8cfb-d37bb269bbb7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-3794d5a6f4de>\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'YOUR_API_KEY'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'path/to/your/image.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0maesthetic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify_outfit_aesthetic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The outfit's aesthetic is: {aesthetic}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-3794d5a6f4de>\u001b[0m in \u001b[0;36mclassify_outfit_aesthetic\u001b[0;34m(image_path, api_key)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \"\"\"\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Load the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimage_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mimage_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path/to/your/image.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Filter and Retrieve Correct Images from Pinecone Database\n",
        "Once the aesthetic is determined, use metadata filtering to query your Pinecone index.\n",
        "Retrieve images that match the classified aesthetic.\n",
        "Ensure the results are diverse and representative of the aesthetic category.\n"
      ],
      "metadata": {
        "id": "QDRFx6d3KukT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "WWCbA03sJgQU",
        "outputId": "0af4821d-8786-440f-ff18-3401c9a1c288"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-093462c3d3e3>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    5. Filter and Retrieve Correct Images from Pinecone Database\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ou_5Lq2QJgVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ledQpU4JgX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8vsvrstIJgaT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}